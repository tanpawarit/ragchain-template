#!/usr/bin/env python3
"""
à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸à¸²à¸£ Ingest à¸‚à¹‰à¸­à¸¡à¸¹à¸¥
à¹à¸ªà¸”à¸‡à¸§à¸´à¸˜à¸µà¸à¸²à¸£à¸›à¸£à¸°à¸¡à¸§à¸¥à¸œà¸¥à¹à¸¥à¸°à¸ªà¸£à¹‰à¸²à¸‡ index à¸ˆà¸²à¸à¹€à¸­à¸à¸ªà¸²à¸£
"""

import os

from src.components.ingestion import DataIngestionPipeline
from src.utils.config.app_config import AppConfig


def main():
    """à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸à¸²à¸£ ingest à¸‚à¹‰à¸­à¸¡à¸¹à¸¥"""
    print("ğŸ“š à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸à¸²à¸£ Ingest à¸‚à¹‰à¸­à¸¡à¸¹à¸¥")
    print("=" * 50)

    # 1. à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹„à¸Ÿà¸¥à¹Œà¸‚à¹‰à¸­à¸¡à¸¹à¸¥
    print("ğŸ“‹ à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¹„à¸Ÿà¸¥à¹Œà¸‚à¹‰à¸­à¸¡à¸¹à¸¥...")

    cfg = AppConfig.from_files("configs/model_config.yaml", "config.yaml")

    # à¸•à¸£à¸§à¸ˆà¸ªà¸­à¸šà¸§à¹ˆà¸²à¸¡à¸µà¹„à¸Ÿà¸¥à¹Œà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆ
    data_folder = cfg.data_folder
    missing_files = []

    for filename in cfg.file_names:
        file_path = os.path.join(data_folder, filename)
        if os.path.exists(file_path):
            file_size = os.path.getsize(file_path)
            print(f"âœ… {filename} ({file_size:,} bytes)")
        else:
            missing_files.append(filename)
            print(f"âŒ {filename} (à¹„à¸¡à¹ˆà¸à¸šà¹„à¸Ÿà¸¥à¹Œ)")

    if missing_files:
        print(f"\nâš ï¸  à¹„à¸¡à¹ˆà¸à¸šà¹„à¸Ÿà¸¥à¹Œ: {', '.join(missing_files)}")
        print("ğŸ’¡ à¸à¸£à¸¸à¸“à¸²à¹€à¸à¸´à¹ˆà¸¡à¹„à¸Ÿà¸¥à¹Œà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹ƒà¸™à¹‚à¸Ÿà¸¥à¹€à¸”à¸­à¸£à¹Œ data/raw/")
        return

    # 2. à¸ªà¸£à¹‰à¸²à¸‡ pipeline
    print("\nğŸ”§ à¸ªà¸£à¹‰à¸²à¸‡ Data Ingestion Pipeline...")
    pipeline = DataIngestionPipeline(cfg=cfg, data_version="latest")
    print("âœ… Pipeline à¸à¸£à¹‰à¸­à¸¡à¹ƒà¸Šà¹‰à¸‡à¸²à¸™")

    # 3. à¹‚à¸«à¸¥à¸”à¹€à¸­à¸à¸ªà¸²à¸£
    print("\nğŸ“– à¸à¸³à¸¥à¸±à¸‡à¹‚à¸«à¸¥à¸”à¹€à¸­à¸à¸ªà¸²à¸£...")
    try:
        documents = pipeline.load_documents()
        print(f"âœ… à¹‚à¸«à¸¥à¸”à¹€à¸­à¸à¸ªà¸²à¸£à¸ªà¸³à¹€à¸£à¹‡à¸ˆ: {len(documents)} à¹€à¸­à¸à¸ªà¸²à¸£")

        # à¹à¸ªà¸”à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸­à¸à¸ªà¸²à¸£
        total_chars = sum(len(doc.page_content) for doc in documents)
        print(f"ğŸ“Š à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”: {total_chars:,} à¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£")

    except Exception as e:
        print(f"âŒ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹‚à¸«à¸¥à¸”à¹€à¸­à¸à¸ªà¸²à¸£à¹„à¸”à¹‰: {e}")
        return

    # 4. à¹à¸šà¹ˆà¸‡à¹€à¸­à¸à¸ªà¸²à¸£à¹€à¸›à¹‡à¸™à¸Šà¸´à¹‰à¸™à¹€à¸¥à¹‡à¸ (Chunking)
    print("\nâœ‚ï¸  à¸à¸³à¸¥à¸±à¸‡à¹à¸šà¹ˆà¸‡à¹€à¸­à¸à¸ªà¸²à¸£...")

    # à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸à¸²à¸£à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸² chunking
    chunking_params = {
        "chunk_size": 1000,  # à¸‚à¸™à¸²à¸”à¹à¸•à¹ˆà¸¥à¸°à¸Šà¸´à¹‰à¸™
        "chunk_overlap": 200,  # à¸ªà¹ˆà¸§à¸™à¸—à¸µà¹ˆà¸‹à¹‰à¸­à¸™à¸—à¸±à¸š
        "use_semantic_chunking": True,  # à¹ƒà¸Šà¹‰ semantic chunking
    }

    try:
        chunks = pipeline.chunk_documents(documents, **chunking_params)
        print(f"âœ… à¹à¸šà¹ˆà¸‡à¹€à¸­à¸à¸ªà¸²à¸£à¸ªà¸³à¹€à¸£à¹‡à¸ˆ: {len(chunks)} à¸Šà¸´à¹‰à¸™")

        # à¹à¸ªà¸”à¸‡à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸Šà¸´à¹‰à¸™à¹à¸£à¸
        if chunks:
            first_chunk = chunks[0].page_content
            print(f"ğŸ“ à¸•à¸±à¸§à¸­à¸¢à¹ˆà¸²à¸‡à¸Šà¸´à¹‰à¸™à¹à¸£à¸ ({len(first_chunk)} à¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£):")
            print(f"   {first_chunk[:200]}...")

    except Exception as e:
        print(f"âŒ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¹à¸šà¹ˆà¸‡à¹€à¸­à¸à¸ªà¸²à¸£à¹„à¸”à¹‰: {e}")
        return

    # 5. à¸ªà¸£à¹‰à¸²à¸‡ FAISS index
    print("\nğŸ” à¸à¸³à¸¥à¸±à¸‡à¸ªà¸£à¹‰à¸²à¸‡ FAISS index...")
    try:
        pipeline.create_and_save_vectorstore(chunks)
        print("âœ… à¸ªà¸£à¹‰à¸²à¸‡ FAISS index à¸ªà¸³à¹€à¸£à¹‡à¸ˆ")
        print(f"ğŸ“ à¸šà¸±à¸™à¸—à¸¶à¸à¸—à¸µà¹ˆ: {pipeline.faiss_index_path}")

    except Exception as e:
        print(f"âŒ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸ªà¸£à¹‰à¸²à¸‡ index à¹„à¸”à¹‰: {e}")
        return

    # 6. à¸—à¸”à¸ªà¸­à¸šà¸à¸²à¸£à¸„à¹‰à¸™à¸«à¸²
    print("\nğŸ” à¸—à¸”à¸ªà¸­à¸šà¸à¸²à¸£à¸„à¹‰à¸™à¸«à¸²...")
    try:
        from langchain_community.vectorstores import FAISS
        from langchain_openai import OpenAIEmbeddings
        from pydantic import SecretStr

        # à¹‚à¸«à¸¥à¸” index à¸—à¸µà¹ˆà¸ªà¸£à¹‰à¸²à¸‡à¸‚à¸¶à¹‰à¸™
        embeddings = OpenAIEmbeddings(
            model=cfg.embedding_model_name, api_key=SecretStr(cfg.openai_token)
        )
        vectorstore = FAISS.load_local(
            pipeline.faiss_index_path, embeddings, allow_dangerous_deserialization=True
        )

        # à¸—à¸”à¸ªà¸­à¸šà¸„à¹‰à¸™à¸«à¸²
        test_query = "à¸¡à¸µà¸„à¸­à¸£à¹Œà¸ªà¸­à¸°à¹„à¸£à¸šà¹‰à¸²à¸‡"
        results = vectorstore.similarity_search(test_query, k=3)

        print(f"ğŸ” à¸„à¹‰à¸™à¸«à¸²: '{test_query}'")
        print(f"ğŸ“Š à¸à¸š {len(results)} à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ:")

        for i, result in enumerate(results, 1):
            content = result.page_content[:150] + "..."
            print(f"   {i}. {content}")

    except Exception as e:
        print(f"âŒ à¹„à¸¡à¹ˆà¸ªà¸²à¸¡à¸²à¸£à¸–à¸—à¸”à¸ªà¸­à¸šà¸à¸²à¸£à¸„à¹‰à¸™à¸«à¸²à¹„à¸”à¹‰: {e}")

    print("\nğŸ‰ à¸à¸²à¸£ ingest à¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸¡à¸šà¸¹à¸£à¸“à¹Œ!")
    print("ğŸ’¡ à¸•à¸­à¸™à¸™à¸µà¹‰à¸„à¸¸à¸“à¸ªà¸²à¸¡à¸²à¸£à¸–à¹ƒà¸Šà¹‰à¸£à¸°à¸šà¸š RAG à¹„à¸”à¹‰à¹à¸¥à¹‰à¸§")


if __name__ == "__main__":
    main()
